"""
ğŸ” SEO åˆ†æå·¥å…·
æä¾›å…¨é¢çš„SEOåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬å…³é”®è¯åˆ†æã€å†…å®¹è´¨é‡è¯„ä¼°ã€æŠ€æœ¯SEOæ£€æŸ¥
"""
import re
import json
from datetime import datetime
from urllib.parse import urlparse
from collections import Counter
import jieba
import requests
from bs4 import BeautifulSoup


class SEOAnalyzer:
    """SEOåˆ†æå™¨"""
    
    def __init__(self):
        self.stop_words = set([
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´',
            'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        ])
    
    def analyze_content(self, content, title="", meta_description="", url=""):
        """
        å…¨é¢åˆ†æå†…å®¹çš„SEOè´¨é‡
        è¿”å›è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        """
        analysis = {
            'score': 0,
            'max_score': 100,
            'issues': [],
            'recommendations': [],
            'keywords': {},
            'readability': {},
            'technical': {},
            'content_analysis': {},
            'meta_analysis': {}
        }
        
        # 1. æ ‡é¢˜åˆ†æ (20åˆ†)
        title_score = self._analyze_title(title, analysis)
        
        # 2. å…ƒæè¿°åˆ†æ (15åˆ†)
        meta_score = self._analyze_meta_description(meta_description, analysis)
        
        # 3. å†…å®¹åˆ†æ (25åˆ†)
        content_score = self._analyze_content_body(content, analysis)
        
        # 4. å…³é”®è¯åˆ†æ (20åˆ†)
        keyword_score = self._analyze_keywords(content, title, analysis)
        
        # 5. å¯è¯»æ€§åˆ†æ (10åˆ†)
        readability_score = self._analyze_readability(content, analysis)
        
        # 6. æŠ€æœ¯SEOåˆ†æ (10åˆ†)
        technical_score = self._analyze_technical_seo(url, analysis)
        
        # è®¡ç®—æ€»åˆ†
        analysis['score'] = title_score + meta_score + content_score + keyword_score + readability_score + technical_score
        
        # ç”Ÿæˆæ•´ä½“å»ºè®®
        self._generate_overall_recommendations(analysis)
        
        return analysis
    
    def _analyze_title(self, title, analysis):
        """åˆ†ææ ‡é¢˜è´¨é‡"""
        score = 0
        
        if not title:
            analysis['issues'].append('ç¼ºå°‘é¡µé¢æ ‡é¢˜')
            analysis['recommendations'].append('æ·»åŠ å…·æœ‰æè¿°æ€§çš„é¡µé¢æ ‡é¢˜')
            return score
        
        title_len = len(title)
        
        # æ ‡é¢˜é•¿åº¦åˆ†æ
        if 10 <= title_len <= 60:
            score += 15
            analysis['meta_analysis']['title_length'] = 'optimal'
        elif 60 < title_len <= 70:
            score += 12
            analysis['issues'].append('æ ‡é¢˜ç¨é•¿ï¼Œå¯èƒ½åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­')
            analysis['meta_analysis']['title_length'] = 'good'
        elif title_len < 10:
            score += 5
            analysis['issues'].append('æ ‡é¢˜è¿‡çŸ­ï¼Œç¼ºä¹æè¿°æ€§')
            analysis['meta_analysis']['title_length'] = 'too_short'
        else:
            score += 8
            analysis['issues'].append('æ ‡é¢˜è¿‡é•¿ï¼Œå°†åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­')
            analysis['meta_analysis']['title_length'] = 'too_long'
        
        # æ ‡é¢˜å…³é”®è¯åˆ†æ
        if self._has_meaningful_keywords(title):
            score += 5
            analysis['meta_analysis']['title_keywords'] = 'good'
        else:
            analysis['recommendations'].append('åœ¨æ ‡é¢˜ä¸­åŒ…å«æ›´å¤šç›¸å…³å…³é”®è¯')
            analysis['meta_analysis']['title_keywords'] = 'needs_improvement'
        
        analysis['meta_analysis']['title'] = title
        analysis['meta_analysis']['title_character_count'] = title_len
        
        return score
    
    def _analyze_meta_description(self, meta_description, analysis):
        """åˆ†æå…ƒæè¿°è´¨é‡"""
        score = 0
        
        if not meta_description:
            analysis['issues'].append('ç¼ºå°‘å…ƒæè¿°')
            analysis['recommendations'].append('æ·»åŠ 120-160å­—ç¬¦çš„å…ƒæè¿°')
            return score
        
        desc_len = len(meta_description)
        
        # å…ƒæè¿°é•¿åº¦åˆ†æ
        if 120 <= desc_len <= 160:
            score += 15
            analysis['meta_analysis']['description_length'] = 'optimal'
        elif 160 < desc_len <= 200:
            score += 12
            analysis['issues'].append('å…ƒæè¿°ç¨é•¿ï¼Œå¯èƒ½åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­')
            analysis['meta_analysis']['description_length'] = 'good'
        elif desc_len < 120:
            score += 8
            analysis['issues'].append('å…ƒæè¿°è¿‡çŸ­ï¼Œå»ºè®®æ‰©å……åˆ°120-160å­—ç¬¦')
            analysis['meta_analysis']['description_length'] = 'too_short'
        else:
            score += 5
            analysis['issues'].append('å…ƒæè¿°è¿‡é•¿ï¼Œå°†åœ¨æœç´¢ç»“æœä¸­è¢«æˆªæ–­')
            analysis['meta_analysis']['description_length'] = 'too_long'
        
        analysis['meta_analysis']['description'] = meta_description
        analysis['meta_analysis']['description_character_count'] = desc_len
        
        return score
    
    def _analyze_content_body(self, content, analysis):
        """åˆ†æå†…å®¹ä¸»ä½“è´¨é‡"""
        score = 0
        
        if not content:
            analysis['issues'].append('å†…å®¹ä¸ºç©º')
            return score
        
        # å†…å®¹é•¿åº¦åˆ†æ
        word_count = len(self._extract_words(content))
        char_count = len(content.replace(' ', '').replace('\n', ''))
        
        if char_count >= 500:
            score += 10
            analysis['content_analysis']['length'] = 'good'
        elif char_count >= 300:
            score += 8
            analysis['content_analysis']['length'] = 'adequate'
        else:
            score += 5
            analysis['issues'].append('å†…å®¹é•¿åº¦ä¸è¶³ï¼Œå»ºè®®å¢åŠ åˆ°300å­—ç¬¦ä»¥ä¸Š')
            analysis['content_analysis']['length'] = 'too_short'
        
        # å†…å®¹ç»“æ„åˆ†æ
        structure_score = self._analyze_content_structure(content, analysis)
        score += structure_score
        
        # å›¾ç‰‡åˆ†æ
        image_score = self._analyze_images(content, analysis)
        score += image_score
        
        analysis['content_analysis']['word_count'] = word_count
        analysis['content_analysis']['character_count'] = char_count
        
        return score
    
    def _analyze_content_structure(self, content, analysis):
        """åˆ†æå†…å®¹ç»“æ„"""
        score = 0
        
        # æ£€æŸ¥æ ‡é¢˜æ ‡ç­¾
        h1_count = len(re.findall(r'#\s+.*', content))
        h2_count = len(re.findall(r'##\s+.*', content))
        h3_count = len(re.findall(r'###\s+.*', content))
        
        if h1_count > 0:
            score += 3
            analysis['content_analysis']['has_h1'] = True
        else:
            analysis['issues'].append('ç¼ºå°‘H1æ ‡é¢˜ï¼Œå»ºè®®æ·»åŠ ä¸»æ ‡é¢˜')
            analysis['content_analysis']['has_h1'] = False
        
        if h2_count > 0:
            score += 2
            analysis['content_analysis']['has_h2'] = True
        else:
            analysis['recommendations'].append('æ·»åŠ H2å‰¯æ ‡é¢˜æ¥æ”¹å–„å†…å®¹ç»“æ„')
            analysis['content_analysis']['has_h2'] = False
        
        # æ£€æŸ¥åˆ—è¡¨
        list_count = len(re.findall(r'^\s*[-*+]\s+.*', content, re.MULTILINE))
        if list_count > 0:
            score += 2
            analysis['content_analysis']['has_lists'] = True
        else:
            analysis['recommendations'].append('ä½¿ç”¨åˆ—è¡¨æ¥æé«˜å†…å®¹å¯è¯»æ€§')
            analysis['content_analysis']['has_lists'] = False
        
        # æ£€æŸ¥é“¾æ¥
        link_count = len(re.findall(r'\[.*?\]\(.*?\)', content))
        if link_count > 0:
            score += 2
            analysis['content_analysis']['has_links'] = True
            if link_count > 10:
                analysis['issues'].append('é“¾æ¥è¿‡å¤šï¼Œå¯èƒ½å½±å“ç”¨æˆ·ä½“éªŒ')
        else:
            analysis['recommendations'].append('æ·»åŠ ç›¸å…³çš„å†…éƒ¨æˆ–å¤–éƒ¨é“¾æ¥')
            analysis['content_analysis']['has_links'] = False
        
        analysis['content_analysis']['heading_structure'] = {
            'h1_count': h1_count,
            'h2_count': h2_count,
            'h3_count': h3_count
        }
        
        return score
    
    def _analyze_images(self, content, analysis):
        """åˆ†æå›¾ç‰‡ä½¿ç”¨"""
        score = 0
        
        # æ£€æŸ¥Markdownå›¾ç‰‡
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        images = re.findall(image_pattern, content)
        
        if images:
            score += 3
            analysis['content_analysis']['has_images'] = True
            
            # æ£€æŸ¥Altæ–‡æœ¬
            images_with_alt = [img for img in images if img[0].strip()]
            if len(images_with_alt) == len(images):
                score += 2
                analysis['content_analysis']['images_have_alt'] = True
            else:
                analysis['issues'].append(f'{len(images) - len(images_with_alt)} ä¸ªå›¾ç‰‡ç¼ºå°‘Altæ–‡æœ¬')
                analysis['content_analysis']['images_have_alt'] = False
        else:
            analysis['recommendations'].append('æ·»åŠ ç›¸å…³å›¾ç‰‡æ¥æé«˜å†…å®¹ä¸°å¯Œåº¦')
            analysis['content_analysis']['has_images'] = False
        
        analysis['content_analysis']['image_count'] = len(images)
        
        return score
    
    def _analyze_keywords(self, content, title, analysis):
        """å…³é”®è¯å¯†åº¦åˆ†æ"""
        score = 0
        
        # æå–æ‰€æœ‰æ–‡æœ¬
        full_text = f"{title} {content}"
        words = self._extract_words(full_text.lower())
        
        if not words:
            return score
        
        # è®¡ç®—è¯é¢‘
        word_freq = Counter(words)
        total_words = len(words)
        
        # æå–å…³é”®è¯
        keywords = {}
        for word, count in word_freq.most_common(20):
            if len(word) > 2 and word not in self.stop_words:
                density = (count / total_words) * 100
                keywords[word] = {
                    'count': count,
                    'density': round(density, 2)
                }
        
        analysis['keywords'] = keywords
        
        # å…³é”®è¯å¯†åº¦è¯„ä¼°
        if keywords:
            score += 10
            max_density = max(kw['density'] for kw in keywords.values())
            
            if max_density > 5:
                analysis['issues'].append(f'å…³é”®è¯å¯†åº¦è¿‡é«˜ ({max_density}%)ï¼Œå¯èƒ½è¢«è®¤ä¸ºæ˜¯å…³é”®è¯å¡«å……')
            elif max_density < 0.5:
                analysis['recommendations'].append('é€‚å½“å¢åŠ ä¸»è¦å…³é”®è¯çš„ä½¿ç”¨é¢‘ç‡')
            else:
                score += 10
                analysis['content_analysis']['keyword_density'] = 'optimal'
        else:
            analysis['issues'].append('æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å…³é”®è¯æ¨¡å¼')
        
        return score
    
    def _analyze_readability(self, content, analysis):
        """å¯è¯»æ€§åˆ†æ"""
        score = 0
        
        if not content:
            return score
        
        # å¥å­åˆ†æ
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return score
        
        # å¹³å‡å¥å­é•¿åº¦
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
        
        if avg_sentence_length <= 20:
            score += 5
            analysis['readability']['sentence_length'] = 'good'
        elif avg_sentence_length <= 30:
            score += 3
            analysis['readability']['sentence_length'] = 'adequate'
        else:
            analysis['issues'].append('å¥å­å¹³å‡é•¿åº¦è¿‡é•¿ï¼Œå»ºè®®ä½¿ç”¨æ›´çŸ­çš„å¥å­')
            analysis['readability']['sentence_length'] = 'too_long'
        
        # æ®µè½åˆ†æ
        paragraphs = content.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        if len(paragraphs) >= 3:
            score += 3
            analysis['readability']['paragraph_structure'] = 'good'
        else:
            analysis['recommendations'].append('å°†å†…å®¹åˆ†ä¸ºæ›´å¤šæ®µè½ä»¥æé«˜å¯è¯»æ€§')
            analysis['readability']['paragraph_structure'] = 'needs_improvement'
        
        # å¤æ‚è¯æ±‡åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        complex_words = len(re.findall(r'[\u4e00-\u9fff]{4,}', content))  # 4å­—ä»¥ä¸Šçš„ä¸­æ–‡è¯æ±‡
        total_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        
        if total_chars > 0:
            complex_ratio = complex_words / total_chars
            if complex_ratio < 0.1:
                score += 2
                analysis['readability']['complexity'] = 'simple'
            else:
                analysis['recommendations'].append('è€ƒè™‘ä½¿ç”¨æ›´ç®€å•çš„è¯æ±‡')
                analysis['readability']['complexity'] = 'complex'
        
        analysis['readability']['avg_sentence_length'] = round(avg_sentence_length, 1)
        analysis['readability']['paragraph_count'] = len(paragraphs)
        analysis['readability']['sentence_count'] = len(sentences)
        
        return score
    
    def _analyze_technical_seo(self, url, analysis):
        """æŠ€æœ¯SEOåˆ†æ"""
        score = 10  # åŸºç¡€åˆ†æ•°ï¼Œå› ä¸ºè¿™é‡Œä¸»è¦åˆ†æURLç»“æ„
        
        if url:
            # URLé•¿åº¦åˆ†æ
            if len(url) <= 100:
                score += 0  # åŸºç¡€åˆ†æ•°å·²ç»™
                analysis['technical']['url_length'] = 'good'
            else:
                analysis['issues'].append('URLè¿‡é•¿ï¼Œå»ºè®®ç¼©çŸ­')
                analysis['technical']['url_length'] = 'too_long'
            
            # URLç»“æ„åˆ†æ
            if re.search(r'[a-zA-Z0-9\-_/]', url):
                analysis['technical']['url_structure'] = 'clean'
            else:
                analysis['recommendations'].append('ä½¿ç”¨æ›´æ¸…æ™°çš„URLç»“æ„')
                analysis['technical']['url_structure'] = 'complex'
        else:
            analysis['technical']['url_analysis'] = 'not_available'
        
        return score
    
    def _generate_overall_recommendations(self, analysis):
        """ç”Ÿæˆæ•´ä½“å»ºè®®"""
        score = analysis['score']
        
        if score >= 80:
            analysis['grade'] = 'A'
            analysis['overall_status'] = 'excellent'
        elif score >= 70:
            analysis['grade'] = 'B'
            analysis['overall_status'] = 'good'
        elif score >= 60:
            analysis['grade'] = 'C'  
            analysis['overall_status'] = 'needs_improvement'
        else:
            analysis['grade'] = 'D'
            analysis['overall_status'] = 'poor'
        
        # ä¼˜å…ˆçº§å»ºè®®
        priority_recommendations = []
        
        if analysis['score'] < 70:
            if not analysis['meta_analysis'].get('title'):
                priority_recommendations.append('æ·»åŠ å¸å¼•äººçš„é¡µé¢æ ‡é¢˜')
            if not analysis['meta_analysis'].get('description'):
                priority_recommendations.append('ç¼–å†™å¼•äººæ³¨ç›®çš„å…ƒæè¿°')
            if analysis['content_analysis'].get('length') == 'too_short':
                priority_recommendations.append('å¢åŠ å†…å®¹é•¿åº¦å’Œæ·±åº¦')
        
        analysis['priority_recommendations'] = priority_recommendations
    
    def _extract_words(self, text):
        """æå–æ–‡æœ¬ä¸­çš„è¯æ±‡"""
        # ä¸­æ–‡åˆ†è¯
        chinese_words = jieba.lcut(text)
        chinese_words = [word for word in chinese_words if len(word) > 1 and word not in self.stop_words]
        
        # è‹±æ–‡åˆ†è¯
        english_words = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
        english_words = [word for word in english_words if word not in self.stop_words]
        
        return chinese_words + english_words
    
    def _has_meaningful_keywords(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å…³é”®è¯"""
        words = self._extract_words(text.lower())
        meaningful_words = [w for w in words if len(w) > 2 and w not in self.stop_words]
        return len(meaningful_words) >= 2
    
    def generate_sitemap_entry(self, url, lastmod=None, changefreq='monthly', priority=0.5):
        """ç”Ÿæˆç«™ç‚¹åœ°å›¾æ¡ç›®"""
        if not lastmod:
            lastmod = datetime.now().strftime('%Y-%m-%d')
        
        return {
            'url': url,
            'lastmod': lastmod,
            'changefreq': changefreq,
            'priority': priority
        }
    
    def analyze_competitor(self, competitor_url):
        """ç«äº‰å¯¹æ‰‹åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            response = requests.get(competitor_url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            analysis = {
                'title': soup.find('title').get_text() if soup.find('title') else '',
                'meta_description': '',
                'h1_count': len(soup.find_all('h1')),
                'h2_count': len(soup.find_all('h2')),
                'image_count': len(soup.find_all('img')),
                'link_count': len(soup.find_all('a')),
                'word_count': len(soup.get_text().split())
            }
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                analysis['meta_description'] = meta_desc.get('content', '')
            
            return analysis
            
        except Exception as e:
            return {'error': f'åˆ†æå¤±è´¥: {str(e)}'}


class SEOReportGenerator:
    """SEOæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_html_report(analysis_data):
        """ç”ŸæˆHTMLæ ¼å¼çš„SEOæŠ¥å‘Š"""
        score = analysis_data['score']
        max_score = analysis_data['max_score']
        grade = analysis_data.get('grade', 'N/A')
        
        # è¯„åˆ†é¢œè‰²
        if score >= 80:
            score_color = 'success'
        elif score >= 60:
            score_color = 'warning'
        else:
            score_color = 'danger'
        
        html_report = f"""
        <div class="seo-report">
            <div class="seo-score-card">
                <h3>SEOæ€»è¯„åˆ†</h3>
                <div class="score-circle">
                    <span class="score text-{score_color}">{score}</span>
                    <span class="max-score">/{max_score}</span>
                    <div class="grade badge bg-{score_color}">{grade}</div>
                </div>
            </div>
            
            <div class="seo-sections">
                <div class="meta-analysis">
                    <h4>å…ƒæ•°æ®åˆ†æ</h4>
                    <div class="analysis-item">
                        <strong>æ ‡é¢˜:</strong> {analysis_data['meta_analysis'].get('title', 'æœªè®¾ç½®')}
                        <span class="badge bg-secondary">{analysis_data['meta_analysis'].get('title_character_count', 0)} å­—ç¬¦</span>
                    </div>
                    <div class="analysis-item">
                        <strong>æè¿°:</strong> {analysis_data['meta_analysis'].get('description', 'æœªè®¾ç½®')[:100]}...
                        <span class="badge bg-secondary">{analysis_data['meta_analysis'].get('description_character_count', 0)} å­—ç¬¦</span>
                    </div>
                </div>
                
                <div class="content-analysis">
                    <h4>å†…å®¹åˆ†æ</h4>
                    <div class="stats-grid">
                        <div>å­—æ•°ç»Ÿè®¡: {analysis_data['content_analysis'].get('word_count', 0)}</div>
                        <div>å­—ç¬¦æ•°: {analysis_data['content_analysis'].get('character_count', 0)}</div>
                        <div>å›¾ç‰‡æ•°é‡: {analysis_data['content_analysis'].get('image_count', 0)}</div>
                        <div>æ®µè½æ•°: {analysis_data['readability'].get('paragraph_count', 0)}</div>
                    </div>
                </div>
                
                <div class="issues-recommendations">
                    <div class="issues">
                        <h4>å‘ç°çš„é—®é¢˜</h4>
                        <ul>
                            {''.join([f'<li>{issue}</li>' for issue in analysis_data['issues']])}
                        </ul>
                    </div>
                    
                    <div class="recommendations">
                        <h4>ä¼˜åŒ–å»ºè®®</h4>
                        <ul>
                            {''.join([f'<li>{rec}</li>' for rec in analysis_data['recommendations']])}
                        </ul>
                    </div>
                </div>
                
                <div class="keywords-analysis">
                    <h4>å…³é”®è¯åˆ†æ</h4>
                    <div class="keywords-list">
                        {''.join([f'<span class="keyword-badge">{kw} ({data["density"]}%)</span>' for kw, data in list(analysis_data['keywords'].items())[:10]])}
                    </div>
                </div>
            </div>
        </div>
        """
        
        return html_report
    
    @staticmethod
    def generate_json_report(analysis_data):
        """ç”ŸæˆJSONæ ¼å¼çš„SEOæŠ¥å‘Š"""
        return json.dumps(analysis_data, ensure_ascii=False, indent=2)