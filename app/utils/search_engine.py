"""
ğŸ” æ™ºèƒ½æœç´¢å¼•æ“
å…¨æ–‡æœç´¢ã€è¯­ä¹‰æœç´¢ã€ç›¸å…³æ¨èå’Œåˆ†ç±»ç­›é€‰
"""
import re
import jieba
import jieba.analyse
from datetime import datetime, timedelta
from collections import Counter
from sqlalchemy import or_, and_, func, desc
from app.models.content import Content, content_tags
from app.models.tag import Tag


class SearchEngine:
    """æ™ºèƒ½æœç´¢å¼•æ“"""
    
    def __init__(self):
        # åˆå§‹åŒ–jiebaåˆ†è¯
        jieba.initialize()
        
        # åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´',
            'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are'
        }
        
        # æƒé‡é…ç½®
        self.weights = {
            'title': 0.4,      # æ ‡é¢˜æƒé‡æœ€é«˜
            'summary': 0.3,    # æ‘˜è¦æƒé‡ä¸­ç­‰
            'content': 0.2,    # å†…å®¹æƒé‡è¾ƒä½
            'tags': 0.1        # æ ‡ç­¾æƒé‡æœ€ä½
        }
    
    def full_text_search(self, query, category=None, page=1, per_page=20, sort_by='relevance'):
        """
        å…¨æ–‡æœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            category: åˆ†ç±»ç­›é€‰
            page: é¡µç 
            per_page: æ¯é¡µæ•°é‡
            sort_by: æ’åºæ–¹å¼ ('relevance', 'date', 'views', 'likes')
        
        Returns:
            dict: æœç´¢ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        if not query.strip():
            return self._empty_result()
        
        # åˆ†è¯å¤„ç†
        keywords = self._extract_keywords(query)
        if not keywords:
            return self._empty_result()
        
        # æ„å»ºåŸºç¡€æŸ¥è¯¢
        base_query = Content.query.filter(Content.is_published == True)
        
        if category:
            base_query = base_query.filter(Content.category == category)
        
        # æ„å»ºæœç´¢æ¡ä»¶
        search_conditions = []
        
        for keyword in keywords:
            # ä¸ºæ¯ä¸ªå…³é”®è¯åˆ›å»ºæœç´¢æ¡ä»¶
            keyword_conditions = [
                Content.title.contains(keyword),
                Content.summary.contains(keyword), 
                Content.content.contains(keyword)
            ]
            search_conditions.append(or_(*keyword_conditions))
        
        # ç»„åˆæ‰€æœ‰æœç´¢æ¡ä»¶
        if search_conditions:
            search_query = base_query.filter(or_(*search_conditions))
        else:
            return self._empty_result()
        
        # è·å–ç»“æœç”¨äºç›¸å…³æ€§è®¡ç®—
        all_results = search_query.all()
        
        # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
        scored_results = []
        for content in all_results:
            score = self._calculate_relevance_score(content, keywords, query)
            scored_results.append((content, score))
        
        # æ’åº
        if sort_by == 'relevance':
            scored_results.sort(key=lambda x: x[1], reverse=True)
        elif sort_by == 'date':
            scored_results.sort(key=lambda x: x[0].created_at, reverse=True)
        elif sort_by == 'views':
            scored_results.sort(key=lambda x: x[0].view_count or 0, reverse=True)
        elif sort_by == 'likes':
            scored_results.sort(key=lambda x: x[0].like_count or 0, reverse=True)
        
        # åˆ†é¡µ
        total = len(scored_results)
        start = (page - 1) * per_page
        end = start + per_page
        paged_results = scored_results[start:end]
        
        # æ„å»ºè¿”å›ç»“æœ
        results = []
        for content, score in paged_results:
            result = {
                'content': content,
                'score': score,
                'highlight': self._generate_highlight(content, keywords)
            }
            results.append(result)
        
        return {
            'results': results,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page,
            'query': query,
            'keywords': keywords,
            'category': category,
            'sort_by': sort_by
        }
    
    def semantic_search(self, query, limit=10):
        """
        è¯­ä¹‰æœç´¢ï¼ˆåŸºäºå…³é”®è¯æå–å’Œæ ‡ç­¾åŒ¹é…ï¼‰
        """
        # æå–æŸ¥è¯¢çš„å…³é”®è¯å’Œä¸»é¢˜
        keywords = jieba.analyse.extract_tags(query, topK=10, withWeight=True)
        if not keywords:
            return []
        
        # åŸºäºå…³é”®è¯æƒé‡æœç´¢
        results = []
        
        # æœç´¢æ ‡é¢˜å’Œå†…å®¹
        for keyword, weight in keywords:
            if keyword in self.stop_words:
                continue
                
            # æœç´¢å†…å®¹
            contents = Content.query.filter(
                Content.is_published == True,
                or_(
                    Content.title.contains(keyword),
                    Content.content.contains(keyword),
                    Content.summary.contains(keyword)
                )
            ).all()
            
            for content in contents:
                # è®¡ç®—è¯­ä¹‰ç›¸å…³åº¦
                semantic_score = self._calculate_semantic_score(content, keywords)
                results.append((content, semantic_score))
        
        # å»é‡å¹¶æŒ‰ç›¸å…³åº¦æ’åº
        unique_results = {}
        for content, score in results:
            if content.id in unique_results:
                unique_results[content.id] = max(unique_results[content.id], score)
            else:
                unique_results[content.id] = score
        
        # è·å–æ’åºåçš„å†…å®¹
        sorted_content_ids = sorted(unique_results.items(), key=lambda x: x[1], reverse=True)
        
        final_results = []
        for content_id, score in sorted_content_ids[:limit]:
            content = Content.query.get(content_id)
            if content:
                final_results.append({
                    'content': content,
                    'semantic_score': score
                })
        
        return final_results
    
    def search_by_tags(self, tag_names, limit=20):
        """æ ‡ç­¾æœç´¢"""
        if not tag_names:
            return []
        
        # è·å–æ ‡ç­¾å¯¹è±¡
        tags = Tag.query.filter(Tag.name.in_(tag_names)).all()
        if not tags:
            return []
        
        tag_ids = [tag.id for tag in tags]
        
        # æŸ¥è¯¢åŒ…å«è¿™äº›æ ‡ç­¾çš„å†…å®¹
        from app import db
        
        results = db.session.query(Content)\
                    .join(content_tags)\
                    .filter(content_tags.c.tag_id.in_(tag_ids))\
                    .filter(Content.is_published == True)\
                    .group_by(Content.id)\
                    .order_by(db.func.count(content_tags.c.tag_id).desc(), Content.created_at.desc())\
                    .limit(limit).all()
        
        return results
    
    def get_related_content(self, content, limit=5, method='mixed'):
        """
        è·å–ç›¸å…³å†…å®¹
        
        Args:
            content: å½“å‰å†…å®¹å¯¹è±¡
            limit: è¿”å›æ•°é‡
            method: æ¨èæ–¹æ³• ('tags', 'category', 'keywords', 'mixed')
        
        Returns:
            list: ç›¸å…³å†…å®¹åˆ—è¡¨
        """
        if method == 'tags':
            return self._get_related_by_tags(content, limit)
        elif method == 'category':
            return self._get_related_by_category(content, limit)
        elif method == 'keywords':
            return self._get_related_by_keywords(content, limit)
        else:  # mixed
            return self._get_related_mixed(content, limit)
    
    def get_trending_content(self, days=7, limit=10):
        """è·å–çƒ­é—¨å†…å®¹ï¼ˆåŸºäºæµè§ˆé‡å’Œæ—¶é—´ï¼‰"""
        since_date = datetime.now() - timedelta(days=days)
        
        results = Content.query\
                  .filter(Content.is_published == True)\
                  .filter(Content.created_at >= since_date)\
                  .order_by(desc(Content.view_count), desc(Content.created_at))\
                  .limit(limit).all()
        
        return results
    
    def get_search_suggestions(self, query, limit=5):
        """æœç´¢å»ºè®®ï¼ˆè‡ªåŠ¨å®Œæˆï¼‰"""
        if len(query) < 2:
            return []
        
        suggestions = []
        
        # åŸºäºæ ‡é¢˜çš„å»ºè®®
        title_matches = Content.query\
                       .filter(Content.is_published == True)\
                       .filter(Content.title.contains(query))\
                       .order_by(desc(Content.view_count))\
                       .limit(limit).all()
        
        for content in title_matches:
            suggestions.append({
                'text': content.title,
                'type': 'title',
                'url': content.get_url() if hasattr(content, 'get_url') else f'/content/{content.id}'
            })
        
        # åŸºäºæ ‡ç­¾çš„å»ºè®®
        if len(suggestions) < limit:
            tag_matches = Tag.query\
                         .filter(Tag.name.contains(query))\
                         .order_by(desc(Tag.usage_count))\
                         .limit(limit - len(suggestions)).all()
            
            for tag in tag_matches:
                suggestions.append({
                    'text': tag.name,
                    'type': 'tag',
                    'url': f'/search?tag={tag.name}'
                })
        
        return suggestions
    
    def get_category_stats(self):
        """è·å–åˆ†ç±»ç»Ÿè®¡"""
        from app import db
        
        stats = db.session.query(
            Content.category,
            func.count(Content.id).label('count'),
            func.avg(Content.view_count).label('avg_views')
        ).filter(Content.is_published == True)\
         .group_by(Content.category)\
         .order_by(desc('count')).all()
        
        return [
            {
                'category': stat.category,
                'count': stat.count,
                'avg_views': round(stat.avg_views or 0, 1)
            }
            for stat in stats
        ]
    
    def _extract_keywords(self, query):
        """æå–å…³é”®è¯"""
        # ä¸­æ–‡åˆ†è¯
        words = jieba.lcut(query.lower())
        
        # è‹±æ–‡åˆ†è¯ï¼ˆç®€å•ç©ºæ ¼åˆ†å‰²ï¼‰
        english_words = re.findall(r'[a-zA-Z]+', query.lower())
        
        # åˆå¹¶å¹¶è¿‡æ»¤
        all_words = words + english_words
        keywords = [word for word in all_words if len(word) > 1 and word not in self.stop_words]
        
        return list(set(keywords))  # å»é‡
    
    def _calculate_relevance_score(self, content, keywords, original_query):
        """è®¡ç®—ç›¸å…³æ€§è¯„åˆ†"""
        score = 0
        
        # æ ‡é¢˜åŒ¹é…
        title_matches = sum(1 for keyword in keywords if keyword in content.title.lower())
        score += title_matches * self.weights['title'] * 10
        
        # æ‘˜è¦åŒ¹é…
        if content.summary:
            summary_matches = sum(1 for keyword in keywords if keyword in content.summary.lower())
            score += summary_matches * self.weights['summary'] * 10
        
        # å†…å®¹åŒ¹é…
        if content.content:
            content_matches = sum(1 for keyword in keywords if keyword in content.content.lower())
            score += content_matches * self.weights['content'] * 10
        
        # æ ‡ç­¾åŒ¹é…
        if content.tags:
            tag_names = [tag.name.lower() for tag in content.tags]
            tag_matches = sum(1 for keyword in keywords if any(keyword in tag_name for tag_name in tag_names))
            score += tag_matches * self.weights['tags'] * 10
        
        # å®Œæ•´æŸ¥è¯¢åŒ¹é…ï¼ˆåŠ åˆ†ï¼‰
        if original_query.lower() in content.title.lower():
            score += 20
        elif content.summary and original_query.lower() in content.summary.lower():
            score += 15
        elif content.content and original_query.lower() in content.content.lower():
            score += 10
        
        # å†…å®¹è´¨é‡åŠ åˆ†
        if content.is_featured:
            score += 5
        
        if content.view_count and content.view_count > 100:
            score += min(content.view_count / 100, 10)  # æœ€å¤šåŠ 10åˆ†
        
        return round(score, 2)
    
    def _calculate_semantic_score(self, content, keywords):
        """è®¡ç®—è¯­ä¹‰ç›¸å…³åº¦"""
        score = 0
        
        for keyword, weight in keywords:
            # æ ‡é¢˜æƒé‡æ›´é«˜
            if keyword in content.title.lower():
                score += weight * 0.5
            
            # æ‘˜è¦æƒé‡ä¸­ç­‰
            if content.summary and keyword in content.summary.lower():
                score += weight * 0.3
            
            # å†…å®¹æƒé‡è¾ƒä½
            if content.content and keyword in content.content.lower():
                score += weight * 0.2
        
        return round(score, 3)
    
    def _generate_highlight(self, content, keywords):
        """ç”Ÿæˆæœç´¢ç»“æœé«˜äº®"""
        highlights = {}
        
        # é«˜äº®æ ‡é¢˜
        highlighted_title = content.title
        for keyword in keywords:
            highlighted_title = re.sub(
                f'({re.escape(keyword)})', 
                r'<mark>\1</mark>', 
                highlighted_title, 
                flags=re.IGNORECASE
            )
        highlights['title'] = highlighted_title
        
        # é«˜äº®æ‘˜è¦
        if content.summary:
            highlighted_summary = content.summary
            for keyword in keywords:
                highlighted_summary = re.sub(
                    f'({re.escape(keyword)})', 
                    r'<mark>\1</mark>', 
                    highlighted_summary, 
                    flags=re.IGNORECASE
                )
            highlights['summary'] = highlighted_summary
        
        # ç”Ÿæˆå†…å®¹ç‰‡æ®µï¼ˆå¸¦é«˜äº®ï¼‰
        if content.content:
            snippet = self._generate_content_snippet(content.content, keywords)
            highlights['snippet'] = snippet
        
        return highlights
    
    def _generate_content_snippet(self, content_text, keywords, max_length=200):
        """ç”Ÿæˆå†…å®¹æ‘˜è¦ç‰‡æ®µ"""
        # å¯»æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content_text)
        
        best_sentence = ""
        max_keyword_count = 0
        
        for sentence in sentences:
            keyword_count = sum(1 for keyword in keywords if keyword in sentence.lower())
            if keyword_count > max_keyword_count:
                max_keyword_count = keyword_count
                best_sentence = sentence
        
        if not best_sentence and sentences:
            best_sentence = sentences[0]
        
        # æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
        if len(best_sentence) > max_length:
            best_sentence = best_sentence[:max_length] + "..."
        
        # æ·»åŠ é«˜äº®
        highlighted_snippet = best_sentence
        for keyword in keywords:
            highlighted_snippet = re.sub(
                f'({re.escape(keyword)})', 
                r'<mark>\1</mark>', 
                highlighted_snippet, 
                flags=re.IGNORECASE
            )
        
        return highlighted_snippet
    
    def _get_related_by_tags(self, content, limit):
        """åŸºäºæ ‡ç­¾çš„ç›¸å…³å†…å®¹"""
        if not content.tags:
            return []
        
        tag_ids = [tag.id for tag in content.tags]
        
        from app import db
        
        related = db.session.query(Content)\
                   .join(content_tags)\
                   .filter(content_tags.c.tag_id.in_(tag_ids))\
                   .filter(Content.id != content.id)\
                   .filter(Content.is_published == True)\
                   .group_by(Content.id)\
                   .order_by(db.func.count(content_tags.c.tag_id).desc(), Content.created_at.desc())\
                   .limit(limit).all()
        
        return related
    
    def _get_related_by_category(self, content, limit):
        """åŸºäºåˆ†ç±»çš„ç›¸å…³å†…å®¹"""
        return Content.query\
               .filter(Content.category == content.category)\
               .filter(Content.id != content.id)\
               .filter(Content.is_published == True)\
               .order_by(desc(Content.view_count), desc(Content.created_at))\
               .limit(limit).all()
    
    def _get_related_by_keywords(self, content, limit):
        """åŸºäºå…³é”®è¯çš„ç›¸å…³å†…å®¹"""
        if not content.content:
            return []
        
        # ä»å½“å‰å†…å®¹æå–å…³é”®è¯
        keywords = jieba.analyse.extract_tags(content.title + " " + content.content, topK=5)
        
        if not keywords:
            return []
        
        # æœç´¢åŒ…å«è¿™äº›å…³é”®è¯çš„å…¶ä»–å†…å®¹
        search_conditions = []
        for keyword in keywords:
            search_conditions.append(
                or_(
                    Content.title.contains(keyword),
                    Content.content.contains(keyword)
                )
            )
        
        if search_conditions:
            related = Content.query\
                     .filter(Content.id != content.id)\
                     .filter(Content.is_published == True)\
                     .filter(or_(*search_conditions))\
                     .order_by(desc(Content.view_count), desc(Content.created_at))\
                     .limit(limit).all()
            return related
        
        return []
    
    def _get_related_mixed(self, content, limit):
        """æ··åˆæ¨èç®—æ³•"""
        related_contents = {}
        
        # åŸºäºæ ‡ç­¾çš„ç›¸å…³å†…å®¹ï¼ˆæƒé‡æœ€é«˜ï¼‰
        tag_related = self._get_related_by_tags(content, limit * 2)
        for item in tag_related:
            related_contents[item.id] = {
                'content': item,
                'score': 10,  # æ ‡ç­¾åŒ¹é…æƒé‡æœ€é«˜
                'reason': 'tags'
            }
        
        # åŸºäºåˆ†ç±»çš„ç›¸å…³å†…å®¹
        category_related = self._get_related_by_category(content, limit)
        for item in category_related:
            if item.id in related_contents:
                related_contents[item.id]['score'] += 5
            else:
                related_contents[item.id] = {
                    'content': item,
                    'score': 5,
                    'reason': 'category'
                }
        
        # åŸºäºå…³é”®è¯çš„ç›¸å…³å†…å®¹
        keyword_related = self._get_related_by_keywords(content, limit)
        for item in keyword_related:
            if item.id in related_contents:
                related_contents[item.id]['score'] += 3
            else:
                related_contents[item.id] = {
                    'content': item,
                    'score': 3,
                    'reason': 'keywords'
                }
        
        # æŒ‰è¯„åˆ†æ’åºå¹¶è¿”å›
        sorted_related = sorted(related_contents.values(), key=lambda x: x['score'], reverse=True)
        
        return [item['content'] for item in sorted_related[:limit]]
    
    def _empty_result(self):
        """ç©ºæœç´¢ç»“æœ"""
        return {
            'results': [],
            'total': 0,
            'page': 1,
            'per_page': 20,
            'total_pages': 0,
            'query': '',
            'keywords': [],
            'category': None,
            'sort_by': 'relevance'
        }


class SearchIndexer:
    """æœç´¢ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.engine = SearchEngine()
    
    def build_search_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•ï¼ˆä¸ºå°†æ¥å…¨æ–‡æœç´¢ä¼˜åŒ–å‡†å¤‡ï¼‰"""
        contents = Content.query.filter_by(is_published=True).all()
        
        index_data = []
        for content in contents:
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            full_text = f"{content.title} {content.summary or ''} {content.content or ''}"
            
            # åˆ†è¯
            keywords = jieba.analyse.extract_tags(full_text, topK=20, withWeight=True)
            
            # æ„å»ºç´¢å¼•æ¡ç›®
            index_entry = {
                'content_id': content.id,
                'title': content.title,
                'category': content.category,
                'keywords': keywords,
                'created_at': content.created_at,
                'updated_at': content.updated_at,
                'view_count': content.view_count or 0,
                'is_featured': content.is_featured
            }
            
            index_data.append(index_entry)
        
        return index_data
    
    def update_content_index(self, content_id):
        """æ›´æ–°å•ä¸ªå†…å®¹çš„ç´¢å¼•"""
        # è¿™é‡Œå¯ä»¥å®ç°å¢é‡ç´¢å¼•æ›´æ–°
        # ä¸ºå°†æ¥çš„æ€§èƒ½ä¼˜åŒ–å‡†å¤‡
        pass


# å…¨å±€æœç´¢å¼•æ“å®ä¾‹
search_engine = SearchEngine()
search_indexer = SearchIndexer()